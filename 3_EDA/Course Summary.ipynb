{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "- Performed to gain better understanding of the data. Does not aim at building a model but to understand data before that\n",
    "- Deals with identifying variables and understanding relationship between them\n",
    "\n",
    "## Data, Data Types & variables\n",
    "### Data Types\n",
    "- Structured -> tabular, rows and columns\n",
    "- Unstructured -> text, image, audio and video\n",
    "### Variable Types\n",
    "- Qualitative (non numerical)\n",
    "    - nominal (ex, name, place etc)\n",
    "    - ordinal (ex, categories that can be sorted (satisified, dissatisfied))\n",
    "- Quantitative (numerical)\n",
    "    - continuous values\n",
    "    - discrete values\n",
    "\n",
    "## Measure of Central tendency\n",
    "- Mean \n",
    "    - Formula: (sum of items)/(total number of items)\n",
    "    - Very sensitive to outliers\n",
    "\n",
    "- Median\n",
    "    - Formula: (total number of items + 1)/2 th item of the sorted list\n",
    "    - Resistant to outliers\n",
    "- Mode\n",
    "    - Frequency of each item\n",
    "    - Highly Resistant to outliers\n",
    "\n",
    "## Measure of dispersion\n",
    "- Variance\n",
    "    - Formula: SUM(\\[x1 - x\\]^2)/total number of items\n",
    "- Standard Deviation\n",
    "    - Formula: SD = SQRT(Variance)\n",
    "- Range\n",
    "    - Formula: Max Value - Min Value\n",
    "- Quertiles\n",
    "    - Q1 \n",
    "        - divides smallest 25% values from larger data\n",
    "        - Formula: SUM(total number of items + 1)/4 th item of sorted list\n",
    "    - Q2 \n",
    "        - divides smallest 50% values from larger data \n",
    "        - Formula: SUM(total number of items + 1)/2 th item of sorted list\n",
    "        - AKA Median\n",
    "    - Q3\n",
    "        - divides larger 75% values from rest\n",
    "        - Formula: \\[SUM(total number of items + 1)*3\\]/4 th item of sorted list\n",
    "    - IQR (Interquertile range)\n",
    "        - Formula Q3-Q1\n",
    "        - represents a box in box plot\n",
    "    - outliers\n",
    "        - 1.5 * IQR \n",
    "- Co efficient of variation\n",
    "    - Greater the value = greater variabtion\n",
    "    - Formula: CV = SD / Mean\n",
    "    - Example: variance is housing price between carrollton and richland hills\n",
    "- Z Score\n",
    "    - How close is the obervation to the Mean\n",
    "    - Formula: Z = (x1 - x)/SD\n",
    "\n",
    "## Summarizing measured data\n",
    "\n",
    "### Five Point Summary\n",
    "- Min\n",
    "- Q1\n",
    "- Q2\n",
    "- Q3\n",
    "- Max\n",
    "\n",
    "### Shape of data\n",
    "\n",
    "# Symmentrical - normal distribution (bell curve)\n",
    "# Skewness \n",
    "    - Positive / Right skewed: Median < Mean\n",
    "    - Negative / Left skewed: Median > Mean\n",
    "    - Formula: SUM\\[(x1 - x)^3\\]/[{n-1} * SD^3]\n",
    "### Co variance\n",
    "- Relationship between two variables\n",
    "- Formula: cov(x,y) = SUM\\[(x1-x) * (y1-y)\\]/total no of items\n",
    "- Positive: x and y goes in the same direction\n",
    "- Negative: x and y goes in different direction\n",
    "- Doesn't imply that  x and y influence each other\n",
    "\n",
    "### Correlation\n",
    "- Relationship between two variables independant of unit (or scale)\n",
    "- Formula:  corr = cov(x,y)/SDx * SDy\n",
    "- Values always in rangeL -1 to 1\n",
    "- Value closer to -1 and 1 -> stronger correlation\n",
    "- Value closer to 0 -> weaker correlation\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic\n",
    "    - data.shape\n",
    "    - data.info()\n",
    "    - data.describe()\n",
    "    - len(data)    \n",
    "    - data.dtypes.value_counts() -- count data types\n",
    "### Measures\n",
    "    - data.mean() | data\\['col1'\\].mean()\n",
    "    - data.median() | data\\['col1'\\].median()\n",
    "    - data\\['col1'\\].mode()\n",
    "    - data.quantile(0.25) | data.quantile(0.50) | data.quantile(0.75) \n",
    "    - IQR = data.quantile(0.75) - data.quantile(0.25)\n",
    "    - Range = data.max() - data.min()\n",
    "    - Variance = data.var()\n",
    "    - SD = data.std()\n",
    "    - Co variance = data.cov()\n",
    "    - Correlation = data.corr()\n",
    "    - data.skew()\n",
    "\n",
    "### Cleaning data\n",
    "    - data.drop('col1', axis = 1, inplace = True)\n",
    "    - data.dropna()\n",
    "    - data.isnull().sum()\n",
    "### Handling Non numerical data\n",
    "    - One Hot Encoding\n",
    "        - df_dummies= pd.get_dummies(data1, prefix='Park', columns=['ParkingArea']) #This function does One-Hot-Encoding on categorical text\n",
    "    - Sklearn OneHotEncoder\n",
    "        - from sklearn.preprocessing import OneHotEncoder\n",
    "        - hotencoder = OneHotEncoder()\n",
    "        - encoded = hotencoder.fit_transform(df_dummies.RegionId.values.reshape(-1,1)).toarray() \n",
    "     \n",
    "### Normalization & Scaling\n",
    "    - StandardScaler (normalizes using z score)\n",
    "        - from sklearn.preprocessing import StandardScaler\n",
    "        - std_scale = StandardScaler()\n",
    "        - std_scale\n",
    "    - MinMaxScalar (normalizes using (x - min)/(max - min))\n",
    "        - from sklearn.preprocessing import MinMaxScaler\n",
    "        - minmax_scale = MinMaxScaler()\n",
    "        - minmax_scale\n",
    "    - Log Transformation\n",
    "        - Used to Transformation large variances into smaller (zoom out)\n",
    "            - import numpy as np\n",
    "            - from sklearn.preprocessing import FunctionTransformer   \n",
    "            - log_transformer = FunctionTransformer(np.log1p)\n",
    "            - log_transformer\n",
    "    - Exponential Transformation\n",
    "        - Used to transform densly populated observation to larger (zoom in)\n",
    "            - exp_transformer = FunctionTransformer(np.exp) # Exponential transform \n",
    "            - exp_transformer\n",
    "\n",
    "### Duplicate values\n",
    "    - dupes = Data.duplicated()\n",
    "    - sum(dupes)\n",
    "    - dupes = Data.drop_duplicates()\n",
    "\n",
    "### Missing values\n",
    "    - Standard missing values - NaN\n",
    "        - Data['Col1'].isnull()\n",
    "        - Data.isnull().values.any()   # Any of the values in the dataframe is a missing value\n",
    "        - Data.isnull().sum().sum()    # Total number of recognised missing values in the entire dataframe\n",
    "        - Data['Number'].fillna(12345, inplace = True)   \n",
    "\n",
    "    - Non standard missing values - bad data or space\n",
    "        - Need to identify based on the context\n",
    "    - Replacing missing values\n",
    "        - Replace with mean\n",
    "        - Replace with median\n",
    "        - Replace with static value\n",
    "        - Self heal the value based on the context\n",
    "        - Data.dropna(inplace=True)\n",
    "        \n",
    "## Correcting outliers\n",
    "1. remove using z-score\n",
    "    - from scipy import stats\n",
    "    - import numpy as np\n",
    "    - z = np.abs(stats.zscore(boston_df))   # get the z-score of every value with respect to their columns\n",
    "    - print(z)\n",
    "    - threshold = 3\n",
    "    - np.where(z > threshold)\n",
    "    - boston_df1 = boston_df\\[(z < 3).all(axis=1)\\]  # Select only the rows without a single outlier\n",
    "    - boston_df2 = boston_df.copy()   #make a copy of the dataframe\n",
    "    - Replace all the outliers with median values. This will create new some outliers but, we will ignore them\n",
    "    - for i, j in zip(np.where(z > threshold)\\[0\\], np.where(z > threshold)\\[1\\]):# iterate using 2 variables.i for rows and j for columns\n",
    "    - boston_df2.iloc\\[i,j\\] = boston_df.iloc\\[:,j\\].median()  # replace i,jth element with the median of j i.e, corresponding column\n",
    "    \n",
    "    \n",
    "2. remove using IQR\n",
    "    - Q1 = boston_df.quantile(0.25)\n",
    "    - Q3 = boston_df.quantile(0.75)\n",
    "    - IQR = Q3 - Q1\n",
    "    - print(IQR)\n",
    "    - np.where((boston_df < (Q1 - 1.5 * IQR)) | (boston_df > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "    - boston_df_out = boston_df\\[~((boston_df < (Q1 - 1.5 * IQR)) |(boston_df > (Q3 + 1.5 * IQR))).any(axis=1)\\] # rows without outliers\n",
    "    - boston_df4 = boston_df.copy()\n",
    "\n",
    "    - Replace every outlier on the lower side by the lower whisker\n",
    "    - for i, j in zip(np.where(boston_df4 < Q1 - 1.5 * IQR)\\[0\\], np.where(boston_df4 < Q1 - 1.5 * IQR)[1]): \n",
    "    - whisker  = Q1 - 1.5 * IQR\n",
    "    - boston_df4.iloc\\[i,j\\] = whisker\\[j\\]\n",
    "\n",
    "## Bivariate analysis\n",
    "- Numerical vs. Numerical\n",
    "    1. Scatterplot\n",
    "    2. Line plot\n",
    "    3. Heatmap for correlation\n",
    "    4. Joint plot\n",
    "- Categorical vs. Numerical\n",
    "    1. Bar chart\n",
    "    2. Voilin plot\n",
    "    3. Categorical box plot\n",
    "    4.Swarm plot\n",
    "- Two Categorical Variables\n",
    "    1. Bar chart\n",
    "    2. Grouped bar chart\n",
    "    3. Point plot\n",
    "## Pandas Profiling\n",
    "- import pandas_profiling \n",
    "- pandas_profiling.ProfileReport(df)\n",
    "- pandas_profiling.ProfileReport(df).to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
